{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3a.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8yNq0XHB9Sq"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def entro(Y=None, pro=None):# calculating entropy\n",
        "\n",
        "    if Y is not None:\n",
        "        pro = sum(Y == 1) / len(Y)\n",
        "    if pro == 0 or pro == 1:\n",
        "        return 0\n",
        "    return -pro*np.log2(pro) - (1-pro)*np.log2(1-pro)\n",
        "\n",
        "\n",
        "def midval(X, Y, A):# calculating a value required for information gain value\n",
        " \n",
        "    c = X[:, A]\n",
        "    return sum(\n",
        "        (sum(c == a) / len(X)) * entro(Y[c == a])\n",
        "        for a in set(c)\n",
        "    )\n",
        "\n",
        "\n",
        "def finalgain(X, Y, A):# calculating information gain\n",
        "\n",
        "    return entro(Y) - midval(X, Y, A)\n",
        "\n",
        "\n",
        "def gainratio(X, Y, A):# calculating the final gain ratio which is further called in DecisionTreeClassifier\n",
        "\n",
        "    c = X[:, A]\n",
        "    loop = set(c)\n",
        "    v = 0\n",
        "    for i in loop:\n",
        "        pro = sum(c == i) / len(X)\n",
        "        if pro == 0 or pro == 1:\n",
        "            return 0\n",
        "        v -= pro*np.log2(pro)\n",
        "    return finalgain(X, Y, A) / v\n",
        "\n",
        "class DecisionTreeClassifier:\n",
        "\n",
        "    class leaf:\n",
        "        def __init__(self, p=None):\n",
        "           \n",
        "            self.p = p\n",
        "            self.A = None\n",
        "            self.c = {}\n",
        "            self.posi = None\n",
        "            self.negi = None\n",
        "\n",
        "        def __repr__(self):\n",
        "            return f\"({self.posi}+, {self.negi}-)\"\n",
        "    \n",
        "    def __init__(self,m_depth):\n",
        "        self.m_depth=m_depth\n",
        "\n",
        "\n",
        "    def fitter(self, X, Y, sample_weights=None):\n",
        "        if sample_weights is None:\n",
        "            sample_weights = np.ones(len(Y))/len(Y)\n",
        "        self.mainr = self.leaf()\n",
        "        self.btree(self.mainr, X, Y, set(), depth=1)    \n",
        "\n",
        "\n",
        "    def prediction(self, x):\n",
        "        assert hasattr(self, 'mainr'), \"not trained yet\"\n",
        "        n = self.mainr\n",
        "        while n.c:\n",
        "            try:\n",
        "                n = n.c[x[n.A]]\n",
        "            except KeyError:\n",
        "                break\n",
        "        return int(n.posi > n.negi)\n",
        "\n",
        "    def error(self, X, Y):\n",
        "        e = np.array([self.prediction(x) for x in X])\n",
        "        return print(sum(e != Y) / len(Y))\n",
        "\n",
        "    def btree(self, mainr, X, Y, attribute, depth):\n",
        "        X, Y, attribute = X.copy(), Y.copy(), attribute.copy()\n",
        "        mainr.posi = sum(Y == 1)\n",
        "        mainr.negi = len(Y) - mainr.posi\n",
        "        if mainr.posi == 0 or mainr.negi == 0:\n",
        "            return\n",
        "        if depth <= self.m_depth:\n",
        "            mainr.A = self.bestfeature(X, Y, attribute)\n",
        "            attribute.add(mainr.A)\n",
        "            if mainr.A is None:\n",
        "                return\n",
        "            for a in set(X[:, mainr.A]):\n",
        "                mainr.c[a] = self.leaf(p=mainr)\n",
        "                self.btree(mainr.c[a],\n",
        "                                 X[X[:, mainr.A] == a],\n",
        "                                 Y[X[:, mainr.A] == a],\n",
        "                                 attribute,\n",
        "                                 depth+1)\n",
        "\n",
        "    def bestfeature(self, X, Y, zat):\n",
        "        best_gain, best_feature = 0, None\n",
        "        for A in range(X.shape[1]):\n",
        "            if A in zat:\n",
        "                continue\n",
        "            gain = gainratio(X, Y, A)\n",
        "            if gain > best_gain:\n",
        "                best_gain = gain\n",
        "                best_feature = A\n",
        "        return best_feature\n",
        "\n",
        "\n",
        "    def show_tree(self):\n",
        "        self.treeprint(self.mainr, depth=0)\n",
        "\n",
        "    def treeprint(self, mainr, depth):\n",
        "        print(mainr)\n",
        "        for k, v in mainr.c.items():\n",
        "            print(\"\\t\"*(depth+1) + f\"Node:[x{mainr.A+1} = {k}] \", end='')\n",
        "            self.treeprint(v, depth+1)\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de0p9zEFva8L"
      },
      "source": [
        "class BoostingTreeClassifier:\n",
        "   \n",
        "    \n",
        "    def __init__(self, n, m_depth):\n",
        "        self.n = n\n",
        "        self.m_depth = m_depth\n",
        "        self.trees = [\n",
        "            DecisionTreeClassifier(m_depth=m_depth)\n",
        "            for i in range(n)\n",
        "        ]\n",
        "        self.alpha = np.ones(n)\n",
        "    \n",
        "    def fitter(self, X, Y):\n",
        "        assert X.shape[0] == Y.shape[0]\n",
        "        N = X.shape[0]\n",
        "        w = np.ones(N) / N\n",
        "        for i in range(self.n):\n",
        "          \n",
        "            self.trees[i].fitter(X, Y)\n",
        "\n",
        "            var2 = np.array([1 if self.trees[i].prediction(x) else -1 for x in X])\n",
        "            e = np.sum(var2 != Y) / len(Y)\n",
        "            e = np.clip(e, 1e-10, 1-1e-10)\n",
        "            self.alpha[i] = 0.5 * np.log((1-e)/e)\n",
        "            \n",
        "            w *= np.exp(-self.alpha[i] * Y * var2)\n",
        "            w /= sum(w)\n",
        "            indices = np.random.choice(np.arange(N), size=N, p=w)\n",
        "            X, Y = X[indices], Y[indices]\n",
        "            \n",
        "    \n",
        "    def prediction(self, x):\n",
        "        var2 = np.array([1 if C.prediction(x) == 1 else -1 for C in self.trees])\n",
        "        if np.dot(var2, self.alpha) > 0:\n",
        "            return 1\n",
        "        else:\n",
        "            return -1\n",
        "    \n",
        "    def accuracy(self, X, Y):\n",
        "        var2 = np.array([self.prediction(x) for x in X])\n",
        "        return print((sum(var2 == Y) / len(Y))*100)\n",
        "    \n",
        "    def error(self, X, Y):\n",
        "        var2 = np.array([self.prediction(x) for x in X])\n",
        "        return sum(var2 != Y) / len(Y)   "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AMx4YNdzkas"
      },
      "source": [
        "Decisiontrain = pd.read_csv('tic-tac-toe_train.csv',header=None)\n",
        "Xtrain = np.asarray(Decisiontrain)[:,:9]\n",
        "Ytrain = (np.asarray(Decisiontrain)[:,-1] == 'win').astype(int)\n",
        "Decisiontest =pd.read_csv('tic-tac-toe_test.csv',header=None)\n",
        "Xtest = np.asarray(Decisiontest)[:,:9]\n",
        "Ytest = (np.asarray(Decisiontest)[:,-1] == 'win').astype(int)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kpuz7UwwzsNW",
        "outputId": "b9dad823-6b0c-4a52-81be-150a236e62d1"
      },
      "source": [
        "obj = BoostingTreeClassifier(10, m_depth=5)\n",
        "obj.fitter(Xtrain, Ytrain)\n",
        "print(\"Error for training data:\",obj.error(Xtrain,Ytrain))\n",
        "print(\"Error for test data:\",obj.error(Xtest, Ytest))\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error for training data: 0.364\n",
            "Error for test data: 0.46\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}